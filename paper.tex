\documentclass[sigconf]{acmart}

\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{framed}
\usepackage{cleveref}
\usepackage{hyperref}


\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[HPDC '19]{HPDC '19: International ACM Symposium on High-Performance Parallel and Distributed Computing}{June 24--28, 2019}{Phoenix, AZ}


\begin{document}
\title[Projecting Globel Filesystems into Local Containers.]{Projecting Global Filesystems into Local Containers for High-throughput Computing on HPC Resources}

\if 0

Abstract

Intro
- CVMFS & HEP applications
- NERSC/HPC resources
- mismatch, new requirements on HPC

Background and Challenges
- CVMFS requirements
- no FUSE, sudo, etc. on HPC
- Frequent updates to CMVFS
- need for past versions and present
- version consistency issues/mismatch with shared filesystem semantics
- DVMFS

-------------------

Identifying Dependencies
- tracing vs. static analysis
- represent deps. as a set of files -> equivalence between the two approaches
- possibility to include more than requested
- WLCG as testbed for tracing
- overhead of tracing
- sampling ratio for live applications

Projecting a Global FS
- projections vs. images
- SquashFS images vs. Docker, ...
- Shrinkwrap
- quantify image size vs. projection size
- data, metadata, cache costs
- creation costs

--------------------

Managing CVMFS Containers at an HPC Site
- multiple snapshots at different versions
- cost of large number of container images (storage, distribution, etc.)
- extreme cases: massive NERSC image vs. loads of tiny containers
- possibility of replacing an image with an augmented version to serve 2 apps
- tradeoffs in space, creation time, etc.
- online problem: create vs. augment

---------------------

PoC Implementation
- consider (path, ID) to deal with versions
- Jacard distance metric
- group images with common parts
- fast to (approximately) compute
- putting too many disjoint pieces in an image pushes it farther and farther away
- LRU to clean up

Evaluation
- sample workloads
- image vs. projection vs. cache size
- space savings
- image re-use
- construction time
- choice of alpha

----------------------

Conclusion

\fi

\author{Tim Shaffer}
\email{tshaffe1@nd.edu}
\orcid{}
\affiliation{%
  \institution{University of Notre Dame}
  \city{Notre Dame}
  \state{Indiana}
  \postcode{46556}
}
  
\author{Nicholas Hazekamp}
\email{nhazekam@nd.edu}
\orcid{}
\affiliation{%
  \institution{University of Notre Dame}
  \city{Notre Dame}
  \state{Indiana}
  \postcode{46556}
}

\author{Jakob Blomer}
\email{jblomer@cern.ch}
\orcid{}
\affiliation{%
  \institution{CERN}
  \city{Geneva}
  \country{Switzerland}
}

\author{Douglas Thain}
\email{dthain@nd.edu}
\orcid{}
\affiliation{%
  \institution{University of Notre Dame}
  \city{Notre Dame}
  \state{Indiana}
  \postcode{46556}
}  


\renewcommand{\shortauthors}{Shaffer et al.}



\begin{abstract}
Scientific applications often depend on large, complex software repositories containing libraries, tools, and frameworks in multiple versions and platforms to support many researchers.
As a practical necessity, software repos are most commonly distributed via shared or global filesystems.
In moving existing research to HPC environments, however,
scientists must work around restrictions that can interfere with global software delivery methods.
To illustrate these challenges,
we examine CVMFS, the CernVM File System.
CVMFS is a global, distributed filesystem that provides read-only access to experiment software and configurations for high energy physics research at CERN.
CVMFS requires a number of network and OS configurations that are incompatible with a restrictive HPC environment.
In this work, we introduce a set of tools and procedures for \emph{projecting} the global filesystem (or parts thereof) into local container images suitable for HPC sites.
As part of our evaluation,
we found that using local approximations of a global filesystem tends to introduce problems of its own in terms of compute and storage overhead.
We thus examined several declarative and profiling-based approaches to creating more fine-grained projections to fit application requirements,
and propose a strategy for efficiently managing these projected container images at an HPC site.
Using our projection tools and image management strategy,
we implemented a proof-of-concept submission system for running existing HEP analysis tasks in an HPC environment.
\end{abstract}


%
% The CCSXML code is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code.
%

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{global filesystem}


\maketitle

\section{Introduction}

Scientific research has a large and rapidly growing need for computation.
Researchers have a number of options at their disposal to meet those needs,
including campus-local or organization-local resources,
cloud providers,
grid computing,
and large HPC sites.
Moving applications between these classes of resources can present problems,
as the assumptions made during development may no longer hold.
For example, organization-local resources and some grid sites commonly provide access to shared filesystems at each compute node.
In addition, there is some flexibility in system configuration to support an organization's primary research activity.
A new computing environment may be much less open to changes for a specific application,
especially large, multi-tenant resources such as HPC sites.
In these cases, it is left to scientists to adapt their application software to the available resources.

Many of our research collaborators have experienced significant frustration in preparing an existing scientific application to run on a new type of resource.
As a concrete example for this work,
we consider the challenges in running simulation and analysis workloads at CERN on HPC sites.
These applications assume several pieces of infrastructure to be in place and require some configuration and customization to the compute environment.
This is not an issue when running on Worldwide LHC Computing Grid~(WLCG),
which is a dedicated resource built to meet CERN's substantial computing needs.
The WLCG currently satisfies most of the computational needs of the LHC experiments at CERN.
With the upcoming high luminosity upgrades to the LHC,
the computational resources needed will will increase by at least an order of magnitude by 2026,
% On CERN's page it estimate 50-100X need, argonne had a report that was 10-20X
prompting researchers to turn to available high performance computing (HPC) resources to meet demand.
%In contrast with the WLCG's single slot submissions,
%HPC applications typically consist of parallel jobs using several cores up to a full machine for each submission.

Most HPC systems, however, have site-specific configuration and security requirements.
These can take the form of limited network availability at computation nodes,
limited container support,
and restricted access to privileged system features (such as FUSE support).
These new restrictions present challenges when porting existing scientific applications,
which expect required software to be readily available,
make a number of assumptions about how the system should be configured,
and may not be designed with tasks such as converting between image and container formats in mind.

In the case of the simulation and analysis workloads at CERN,
unrestricted network access and FUSE support are critical for providing global filesystem access.
Nearly all research software used at CERN assumes the availability of the CernVM File System (CVMFS),
a global filesystem built for distributing software and configuration data.
CMVFS provides a read-only, distributed file system using HTTP for network transport and extensive local
caching to rapidly distribute updates in software across the WLCG.
Dependence on CVMFS to distribute experiment software currently makes it impossible to take advantage of many HPC sites.
CVMFS requires unrestricted network access and is implemented as a FUSE module,
both of which sites often disallow for security and management reasons.

Some sites, such as NERSC,
have attempted workarounds for providing CVMFS,
such as creating large images several terabytes in size that contain all of the software stored in CVMFS or
maintaining local mirrors of CVMFS available via site-local shared filesystems.
However, both of these existing solutions require 
hours to days of attention from system administrators to deploy or update,
which leads to lag between the available software and the current version.
Additionally for systems that utilize large container images, large amounts of storage are needed to maintain different versions.

Since each LHC experiment job uses 
only a relatively small portion of CVMFS,
a more fine-grained approach to preparing container images would reduce the time spent creating images and allow for more precise control of the software versions in use.
This could also reduce the storage required at each worker node and the network bandwidth spent transferring images.
It is often possible to determine dependencies for particular applications statically by examining submission commands, setup scripts, etc.
This approach, while reliable, tends to be labor-intensive and tied to specific experiments and software stacks.
We therefore modified the reference implementation of CVMFS to allow us to capture the filesystem interactions of live HEP applications.
Using live tracing and static analysis,
we are able to determine exactly which parts of the \texttt{/cvmfs} file system tree CVMFS must be included in a container image for a particular job.
We developed a tool, Shrinkwrap, that can efficiently \emph{project} CVMFS (or a subset thereof) at a particular version into a traditional filesystem.
Such a \emph{projection} of CVMFS is suitable for inclusion in a static container image that does not depend on FUSE or network access.

While working with projected container images and Shrinkwrap,
we found that the cost of creating, distributing, and storing customized images for a large number of jobs and users creates unexpected challenges as well.
Shrinkwrap is designed to efficiently manage projections by deduplicating data in the filesystem.
Disk images, however, must contain complete copies of all included data.
The na\"{i}ve approach of creating a projection of CVMFS for each application results in frequent duplication of data and significant overhead in terms of computation and bandwidth.
Depending on the granularity of the projection,
image creation time can easily exceed individual job's actual compute time.
The alternative extreme, i.e.\ building a single large image that satisfies the requirements of all applications,
forces all jobs to use the same repository revision,
which can result in incompatibilities across applications.
Updates to the repository or changes in applications and requirements can again result in large amounts of overhead due to frequently creating, transferring, and storing these very large images.

Observing that jobs frequently have significant overlap in dependencies,
we designed a system to more efficiently manage container images on job submission.
First, we defined a metric for determining the similarity between projections.
This gives the possibility of identifying existing container images that could be re-used to fulfill a new request.
We also determined that in some cases,
it is less expensive to augment an existing, similar projection to allow a single container image to support multiple distinct applications.
We bring these tools together as a proof-of-concept submission system for making online decisions about whether to create, reuse, or augment container images to efficiently support high-throughput HEP jobs on HPC resources. 

Compared to existing solutions,
Shrinkwrap created more compact projections than traditional methods such as rsync copies,
and allowed for more efficient updates to existing projections.
Based on the existing projects to produce static CVMFS container images and traces of benchmark applications,
we examined the sizes and structures of several projected container images.
Based on these properties,
we simulated the behavior of our image management system over a large number of job submissions.
We found that our approach can effectively manage the container images for a continuous stream of submissions with varying software dependencies and versions with less storage overhead than creating a new image per application.
In addition, our results show that applications can still take advantage of precise dependency and version specifications to reduce image creation and transfer overhead compared to working with large/complete mirrored copies of CVMFS.

\section{Background and Challenges}

\subsection{CVMFS}
The CernVM File System (CVMFS) is a globally distributed filesystem designed for providing efficient,
read-only access to scientific software.
CVMFS is primarily used for distribution of High Energy Physics (HEP) software used as part of the Large Hadron Collider (LHC) at CERN.
Researchers at CERN use CVMFS as the primary means of distributing the analysis and simulation software they develop to the Worldwide LHC Computing Grid (WLCG).

Each WLCG site commits to providing some fixed amount of computational resources for analyzing particle collision events and running simulations.
Worker nodes at each site must be configured to mount CVMFS as a prerequisite to performing any computational tasks.
Since CVMFS is implemented as a FUSE module,
WLCG sites are expected to support FUSE on their worker nodes.
CVMFS also relies on global configuration files,
requiring administrative involvement in preparing worker nodes for WLCG jobs.
In addition, sites generally provision some infrastructure to support CVMFS,
such as caching proxies to reduce load created by local workers on publicly accessible filesystem servers.

As a result, it is difficult to contribute computational resources toward the LHC experiments without making some site-wide provisioning and configuration changes.
This is not an issue for the WLCG sites,
as they provide dedicated computing resources and can customize their setups to meet the requirements of CVMFS.
For a professor attempting to harness a campus cluster,
on the other hand, these requirements present a serious barrier.
Solutions exist such as
Parrot, a debugging tool and personal filesystem developed at the University of Notre Dame,
which sidesteps FUSE but still requires unrestricted network access and
UnCVMFS which locally store all of CVMFS to avoid network access, 
but relies on large storage and permission mounting.
For computing at scale, it is important to provide a caching proxy and concise storage,
which currently require assistance from administrators.

\subsection{HPC}

With the rapidly increasing demands of the LHC experiments at CERN,
HPC resources are an appealing source of computing power to supplement the WLCG.
Leadership class machines such as Summit, at Oak Ridge National Laboratory, will by themselves be able to perform a substantial portion of the work of the entire WLCG today.
Use of national-scale HPC resources across the United States and Europe such as Cori at NERSC, Sierra at Lawrence Livermore National Laboratory, Piz Diant at Swiss National Supercomputing Centre could be key in meeting the computational demands following the LHC's high-luminosity upgrade.

Unfortunately, HPC sites are generally more restrictive than the previously mentioned campus resources.
In addition to limited privilege on workers and lack of FUSE support,
HPC sites often impose restrictions on network activity,
including outbound firewalls preventing access to CVMFS.

The US collaboration of the ATLAS project is currently taking advantage of computing resources at various supercomputers in the United States including Cori at NERSC.
For the deployment of their software the \texttt{uncvmfs} utility is used to unpack entire CVMFS repositories into standalone images (usually in ext4 or squashfs format) which in turn can be used to build Shifter or Singularity images.
While these images can very well be scaled out onto a large number of nodes inside the NERSC infrastructure,
the image build process takes around 24 hours which makes it difficult to deploy up-to-date versions of the software on a regular basis.
As these images contain complete copies of everything in CVMFS,
each takes on the order of terabytes,
necessitating careful storage management.
In addition, the process requires administrators for image creation, deployment, and cleanup.
Currently, this approach is used in production on the ATLAS and CMS experiments.
As additional experiments want to take advantage of the resources at NERSC,
the administrative burden of managing multiple CVMFS images on multiple software versions increases accordingly.
For the experiments to use the same images (saving time and cost at NERSC),
multiple groups would have to come to agreement about a single common set of software versions to use and adjust their pipelines elsewhere.
Both solutions introduce new inconveniences and administrative burden.

More broadly, the use of static, manually managed container images breaks user assumptions about the operation of CVMFS.
When running jobs using CVMFS,
researchers can specify the version of each repo to use,
allowing precise control over software and dependencies.
Each experiment has its own policies and procedures for choosing a software version;
some may use bleeding-edge analysis software or another experiment may prefer older, stable versions.
CVMFS is able to provide multiple customized views of the global filesystem without incurring much additional cost.
CVMFS also supports some more exotic features,
such as symlink resolution controlled by environment variables.
CVMFS as a whole does not map well to the semantics of a traditional filesystem.
Projecting CVMFS into such a filesystem can capture at most a single, static version of a repo and must necessarily eliminate some features,
such as environment-based symlink resolution at runtime.

\if 0
% https://indico.cern.ch/event/587955/contributions/2937411/attachments/1683156/2705423/WahidBhimji-CHEP18-NERSC.pdf
Another similar approach in use at NERSC is DVMFS.
Since creating static container images containing CVMFS is costly and difficult to customize,
DVMFS instead uses Cray DVS IO forwarders to mount a copy of CVMFS served over NFS.
This allows for less administrator involvement,
since the mirrored copy of CVMFS can be automatically updated on a regular basis.
After dealing with several implementation problems,
DVMFS is currently deployed at NERSC as an alternative to static images.
While this approach affords more flexibility in choice of repos and software versions,
it increases the job startup time and places additional requirements on the infrastructure.
To be usable in production, DVMFS requires a sufficient number of DVS servers to be provisioned.
DVMFS is nonetheless a viable solution as long as the costs of developing/maintaining the service and allocating infrastructure do not grow too high.
\fi

\section{Defining Application Dependencies}

While the entirety of CVMFS is fairly large,
a single application would never use every piece of data.
Rather than attempting to provide a complete copy of CVMFS locally,
which entails significant storage costs,
we could instead provide a limited subset of the complete filesystem that is sufficient for a particular job.
This approach is more in line with traditional software packaging practices,
where software is provided as needed in the form of RPM files or another similar format.
Working with software packages is simpler in that dependencies are explicitly specified in the package file or database.
With CVMFS, no such definitive specification exists.
This issue is common to shared/global filesystems,
since users expect the complete contents to be available on demand and so devote little time to explicitly defining dependencies and other requirements.

In the case of CVMFS,
building an accurate 
specification of the files, libraries, and packages
needed for each experiment is the first step toward creating usable container images.
Such file specifications can be used when 
projecting CVMFS into realized filesystems.
The correctness and storage overhead of the projected filesystem hinges on the clarity and accuracy of the file specification.
File specifications can be derived from several
different sources, such as 
usage traces, static analysis, or package listings.
The end goal of any of these methods is to 
create a comprehensive list of needed files and structure.
The nature of global filesystems flexible behavior
further extends file specifications by requiring
not just the files and structure, but also what
version/revision was used to verify the contents as well.

\subsection{Creation of Specification}
File specifications can be created several ways,
with the obvious option being handwritten.
However, this quickly becomes unfeasible with
even a moderate number of requirements.
This leaves the need for an automate method to create
specifications.

\subsubsection{Tracing}
The first method is to create a sample trace for your
jobs. 
Creating a trace from a sample job allows 
for a listing of actual files used, which
can be more accurate than handwritten listings.
As these jobs are typically on the order of thousands
to millions of jobs, sampling a small subset
can provide good software coverage with little
additional overhead. % the computation is still used
The burden of creating these traces is
dependant on the method used,
which could be an LD\_PRELOAD library,
using Parrot to track used files, % I think parrot was mentioned previously
or building the tracing in to the application.

Tracing is attractive as it provides a consistent 
behavior between experiments and use cases.
In addition, it is easier to automate and sample arbitrary applications without assistance from each user.
This allows the solution to port between applications
quickly without having to delve in to the applications
specific configuration.
As previously mentioned, there are several methods for
obtaining a trace from a file.

Despite assumption that both CVMFS and FUSE will be limited
on the HPC site, this methods was used as it is reasonable to
allow for sampling of these jobs on WLCG. 
WLCG provides the flexibility of using CVMFS and FUSE, without
having to implement or provision a separate tracing infrastructure
at each site.
Additionally, as many of these jobs would have originally been 
destine to run on WLCG, the sampling sizes are unlikely to 
create significant overhead.
Using CVMFS for the tracing allows CVMFS's just-in-time deployment
of files to patch the environment and give an accurate view without
the risk of missing files.

\begin{figure}
\includegraphics[width=\linewidth]{curated/cdf/cdf.pdf}
\caption{The distribution of file size for each repository.
From this we can see that the large majority of files
accumulate in the kilobyte to megabyte range.}
\label{fig:cdf}
\if 0
lots of small files
table shows duplication
long tail of large files
different scales, similar usage across repos

\fi
\end{figure}

\subsubsection{Packages}
The second automated method is to use static
analysis for discover requirements.
This would allow the application to be analyzed
prior to running to determine what packages are
linked.
Static analysis can be done at several levels,
each providing different levels of control 
and flexibility.
This could be more language specific in the 
form of following import lines in python or
include lines in C.
The alternative is to use repository specific 
methods for setting up the environment,
such as bash scripts for pointing to correct 
software versions, or 
analyzing SCRAM\cite{DBLP:journals/corr/cs-OH-0306014} lines
to determine the configuration.
In the SFT CVMFS repository, where many core research packages are located,
dependencies are mapped for many packages using buildinfo files.
Chaining these buildinfo files together allow for a rough dependency
tree to be create, which was used to sample for \Cref{fig:picks}.
With either of these methods the issues
that arise come from bleeding edge or
experiment specific code that is not directly
handled by the configuration.
There is a significant mix between languages,
configurations methods, and software organization
practices even within a single repository,
which compounds the above issues in this method.

An combination of tracing and static analysis is the
use of package listings. 
Package listings provide a high level indication
of the software needed, and allow for 
flexibility on the specifying user about the
exact files needed.
Using high level packages provides a method
for handwritten/tuned specifications that 
include all aspects of the package.
The downside of using packages is that in
their flexible, forgiving specification
they can be bloated and include unnecessary
files and data.
Packages also require that there is consistent
software organization, which often differs
between repositories.

\begin{figure}
\includegraphics[width=\linewidth]{curated/closure/closure.pdf}
\caption{Difference in number of packages and size when accounting for dependency based closure in selection.}
\label{fig:picks}
\if 0
you get more than you ask for
smaller requirements have relatively large responses
non-linear, so bigger ones have more duplicates
appraoches repo size in limit
merging is subject to drop-off(more duplication)
\fi
\end{figure}



\subsection{Unique Consistency Requirements}
CVMFS differs from traditional POSIX filesystems in that previous versions of all content remain available.
A researcher may specify the precise revision under which an application is known to run.
Thus the same path may refer to two completely different files for a pair of applications.
This is beneficial from the standpoint of reliability and reproducibility,
but complicates the process of projection.
A file specification needs to include enough information
to establish the directory structure and verify the contents at a specific revision.
This can be done in several ways, 
such as computing the checksum of contents,
using global revision numbers, 
or running verification software.
Computed checksums provide a system agnostic approach
to verification by allowing the checksum to be computed
at specification and verified before use.
Computed checksums provide a
consistency guarantee to the byte level,
but are often expensive at runtime for large
production systems.
This approach is alleviated in systems such as 
Git and CVMFS as the contents' checksums are 
computed at ingestion into the repository.
This method relies on the underlying system computing
and providing this information for verification.

\begin{table*}[t]
\begin{center}
\begin{tabular}{|c|r r r|r r r|}
\hline
CVMFS & & ls -lr & & & Shrinkwrap & \\
Repository & Total Size (MB) & inode Count & Time to walk & 
  Data Files & Deduplicated Data Files & Deduplicated Data (MB) \\ \hline
%alice-ocdb & 482,642,307,428 & 1,621,661 & 2:22 &
% 1,621,282 & 1,595 & 24,433,131,203\\
%alice & 3,980,875,747,986 & 27,322,350 & 51:53 & 
%  797,404 & 23,996,541 & 2,098,509,015,172\\
alice & 3,980,875 & 27,322,000 & 51:53 & 
  797,000 & 23,997,000 & 2,098,509\\
%atlas-condb & 719,548,941,240 & 11,781 & 0:04 &
%  10,940 & 229 & 471,492,086 \\
%atlas & 6,195,013,689,056 & 120,598,601 & 3:48:07 &
% 8,823,179 & 85,554,444 & 3,760,712,059,548 \\
atlas & 6,195,013 & 120,599,000 & 3:48:07 &
 8,823,000 & 85,554,000 & 3,760,712 \\
%boss & 120,048,527,315 & 2,418,025 & 4:18 &
% 256,061 & 1,868,192 & 72,301,955,666\\
boss & 120,048 & 2,418,000 & 4:18 &
 256,000 & 1,868,000 & 72,302\\
%cms & 581,789,406,847 & 26,224,567 & 1:07:30 &
% 18,667,178 & 101,448,867 & *2,242,658,825,701\\
cms & 581,789 & 26,225,000 & 1:07:30 &
 18,667,000 & 101,449,000 & *2,242,658\\
%geant4 & 297,571,283,648 & 7,128,024 & 10:36 &
% 323,280 & 6,219,891 & 68,381,969,660 \\
geant4 & 297,571 & 7,128,000 & 10:36 &
 323,000 & 6,220,000 & 68,382 \\
%grid & 30,642,826,259 & 714,235 & 1:17 &
% 141,622 & 583,923 & 18,804,003,040 \\
grid & 30,643 & 714,000 & 1:17 &
 142,000 & 584,000 & 18,804 \\
%lhcb & 2,134,893,079,440 & 33,736,802 & 1:01:26 &
% 10,032,339 & 20,406,235 & 368,070,277,634 \\
lhcb & 2,134,893 & 33,737,000 & 1:01:26 &
 10,032,000 & 20,406,000 & 368,070 \\
%na61 & 5,222,967,734 & 25,602 & 0:02 &
% 8,609 & 15,001 & 2,425,507,431 \\
na61 & 5,222 & 26,000 & 0:02 &
 9,000 & 15,000 & 2,426 \\
%sft & 3,996,527,275,237 & 128,608,792 & 4:17:58 &
% 6,821,699 & 63,210,139 & 2,016,609,810,573 \\ 
sft & 3,996,527 & 128,609,000 & 4:17:58 &
 6,822,000 & 63,210,000 & 2,016,610 \\ 
 \hline
\end{tabular}
\caption{CVMFS repository sizes.}
\label{tab:repo-sizes}
This table shows the size, number of inodes (rounded to nearest thousand), and the amount
of duplicate data that is eliminated using CVMFS.
\end{center}
\end{table*}

\section{Realization of CVMFS Projection}

\subsection{Definition of Projection}

In the case of global filesystems,
every time the filesystem is used locally
the flexible global state has to be 
locked for that concrete use case.
In the case of CVMFS,
this includes specifying the date-time
or revision number of the repository.
This state is used to determine 
what the global filesystem's directory
structure and file contents are, 
allowing for future use of the same state.
CVMFS is able to flexibly move
between these revisions using the FUSE module
to determine the structure and files and
relies on HTTP to pull any missing files.
However, with neither the HTTP access or
FUSE permissions, a method is needed to 
create and track realizations of this
state for future use.

We call these realizations of the global filesystem
projections.
A projection is a concrete filesystem image that can be
mounted, loaded, or copied into an existing system.
Projections can then be used to run experiments
and analysis using the global filesystem without
the previously mentioned permissions.
Projections have been used with CVMFS before,
in the form of unCVMFS and rsync.
UnCVMFS is a utility that projects an entire
CVMFS repository, at a set revision, into the local
filesystem. 
UnCVMFS utilizes CVMFS's data/file separation
to deduplicate the underlying data, but requires
entire repositories to be projected.
When used with several repositories for large projects,
unCVMFS can easily consume 500GB for single projection.
As is often the case, different experiments/analyses 
require different revisions that may conflict.

Rsync allows for file lists to specify and download
subsets of CMVFS repositories, but because it
was designed for general filesystem synchronization
rsync flattens the files, introducing duplicate data.
This direct representation of the file tree also
excludes the possibility of shared data repositories
between different projections.

% Probably need to mention that many applications assume
% CVMFS software is installed/mounted at /cvmfs

\begin{figure}[h]
\includegraphics[width=\columnwidth]{drawings/shrinkwrap-structure.png}
\caption{Shrinkwrap Approach}
\label{figure:shrinkwrap-arch}
\end{figure}

We built the CVMFS Shrinkwrap utility,
a solution that leverages the best of both
unCVMFS and rsync.
Shrinkwrap uses the CVMFS's client interface to
create and update CVMFS projections.
Shrinkwrap uses a CVMFS configuration to specify the
revision of the repository, a file-specification list
to determine the repository subset to project, and
deduplicates the data in a user specified location.
Shrinkwrap can be configured to share the data
repository between several repositories and even
projections of the same repositories.
This allows for multiple realized projections
to coexist, with the desired projection
mounted for use.
In \Cref{figure:shrinkwrap-arch} you can see an outline
of the design of the Shrinkwrap utility.

\subsection{Packaging Projections}

All of the above methods described utilize CVMFS
and rely on the internet connection to create
the projections.
A core assumption is that external network
access is limited at the compute nodes, so a method
is needed to transform and use these projections
for computation.
For this several methods for providing
these projections using 
Container images (images specific to Docker, Singularity, ect.)
or SquashFS images can be used.
The goal is to find a flexible, compatible intermediate
format that allows projections to be moved between sites
and technologies.
For this purpose, Docker images, Singularity images, and 
SqaushFS are interchangeable as outputs of the Shrinkwrap process
and changed as needed for each use case as seen below.

%are compared on their time to create,
%storage requirements, and most importantly compatibility
%with several used technologies, namely 
%Docker, Singularity, and Shifter.

The first aspect is the creation of these images.
From a defined projection of a repository
what are the steps needed to create each of these image types.

\subsubsection{Atlas at NERSC}

As previously described, NERSC is working in collaboration the LHC ATLAS project
to provide HPC resources for simulation and experimentation.
NERSC is currently deploying CVMFS images as monolithic container 
built using UnCVMFS, packaged in Squashfs images, and then containerized 
into Shifter or Singularity format, whose final format
relies on the target site.
This entire process can take up to a day to complete with the majority
of compute used to pull down and store the full CVMFS copy and storage
needs in excess of terabytes for images.
By combining either traced samples or statically defined requirements for 
jobs, Shrinkwrap can reduce these images orders of magnitudes.
Shrinkwrap which relies on the libcvmfs library reduces 
the total overhead and eliminates unnecessary files transfers
over UnCVMFS by allowing selections and rsync using data deduplication.

\subsubsection{Benchmarking Containers}
For benchmarking purposes, the CERN IT-Department is interested in a ``Benchmark in a Container'' solution which allows the distribution of standalone container images for system performance evaluations. For such benchmarks the IT-Department typically relies on simulation workflows provided by CERN's experiment collaborations. To execute these benchmarks in an easily portable and reproducible manner, it is of high interest to package these software workflows into containers. Since the workflows' software stacks are usually stored inside CVMFS repositories, this is a perfect use case for the shrinkwrap utility and its Docker injection functionality.

For the image export the workflow’s resource-needs would be examined through the CVMFS tracing feature in a first step. This would then result in a log containing detailed insights on the paths which were accessed by the benchmarking workload. In a second step we can then produce a specification based on the tracing log which will allow an easy, efficient export of the necessary files from CVMFS. Afterwards the Docker injector can be used to inject the exported files into a prepared benchmarking container stored in a registry. Later, the same utility can be used to update the current container to an up-to-date software version or — depending on the exact use case — the utility could even be integrated into a continuous deployment mechanism which produces new benchmarking containers on every software update.

\section{Managing Projections as a Site-wide Resource}

With static analysis and dynamic tracing of tasks,
we have methods to choose subsets of CVMFS to project to run specific tasks.
Users can collect this information themselves,
or tracing, too, could be handled automatically.
We have seen two approaches to this in section ??.
For applications already running on the WLCG,
it is also possible to instrument a fraction of runs to build a profile of the type of task.
Alternatively, we could integrate tracing with image creation by automatically running a small number of sample tasks on the WLCG or on dedicated bridge nodes and collecting traces as necessary.
These approaches give a fair amount of flexibility in determining accurate dependency information.
Based on this information,
we are interested in automatically providing suitable projections of CVMFS to users.

Shrinkwrap provides a mechanism for projecting parts of CVMFS,
but is not by itself a complete solution as it does not integrate with job submission or manage images after creation.
In the case of a small number of manually created container images,
this is not a serious problem.
Administrators could manage a small collection of images in this case and adjust jobs to work with their setup.
Thus for integrating with the existing systems at NERSC,
Shrinkwrap alone might be sufficient.
To support CVMFS as a site-wide service with a potentially varied user base and workload,
a more automated solution
(e.g.\ a batch system plugin that can automatically prepare container images on job submission)
is required.
This both minimizes direct administrator burden and more closely approximates user experience on other resources like the WLCG.
Our first attempt at such a system,
described in the next sections,
took the simplest possible approach to image.
As we were developing this approach, however,
a number of issues in real-world usage led us to conculde that a somewhat more sophisticated solution was required.

\subsection{Simulating Job Submissions}

Since there are currently few regular CVMFS on HPC users,
we decided to simulate the activity of users requesting container images.
We decided to simulate at the granularity of packages rather than individual files,
because this results in more robust container images.
As mentioned previously,
fine-grained tracing runs the risk of leaving out important dependencies.
In addition, working only based on the properties of individual files ignores the structure inherent in software collections.
Looking only at the properties of individual files,
Figure~\ref{fig:cdf} shows cumulative distributions for file sizes in several of the largest and most active CVMFS repos.
The repos vary in scale,
but all show similar usage patterns.
We see that across all repos,
the majority of files are small (between 1~KB and 1~MB).
The repos do contain, however,
a small number of large files.
This fits our expectation that the CVMFS repos primarily contain software.
Most individual files are small,
but the repo size is defined by the large number of packages in the software collection.

We tested two schemes for generating image requests taking into account the properties of the software collections,
as well as recorded user activity patterns.
We also generated images via a uniform random scheme for comparison.
We consulted with the developers of CVMFS,
as well as HEP researchers at the University of Notre Dame collaborating with CERN to determine how current users interact with CVMFS.
Since CVMFS is not currently widely deployed on HPC resources,
the prospective user base for our container management system is limited.
We were, however, informed of some researchers and groups experimenting with static mirrors of CVMFS for container images,
including projects at NERSC and Stanford University,
with one of the setups available publicly at \url{https://github.com/wyang007/atlas-fat-container}.

To evaluate the effect of including dependencies in container images,
we made a large number of random selections of packages from the SFT repo and then compared with the set of dependencies required by each selection.
Figure~\ref{fig:picks} shows the results over a large number of selection sizes.
We observe of course that selection size increases uniformly along the $x$ axis.
The on-disk size of the selections appears to increase proportionally to selection size.
Recursively including the dependencies of these same selections,
however, results in a significant increase in number of packages and size on disk.
For small selections (less than 100~packages),
the complete images with dependencies might contain 5x as many packages as requested,
with storage increasing proportionally.
With larger selections, however,
this increase becomes less prominent.
This non-linear change is the result of the tree structure of the software dependencies.
There are a number of core components that are transitive dependencies of a large number of packages.
These common dependencies are only included once,
so subsequent selections that depend on them do not add them again.
As the number of selections increases,
this curve will approach the total repo size.
Also note that the non-linear increase in size with increasing selection size means that the size of combining two selections of 100 packages (and their dependencies) is not as simple as adding them together.
Due to duplication, we would expect the resulting combined size to be lower,
as if choosing 200 packages from the start.


\if 0
spread across everyone, lot of commonality/overlap
few with very high usage, very large number of rarely used
lot not observed
\fi

Based on correspondence with researchers,
we made some general observations about the behaviors and requirements of applications we need to support.
First, there are some components such as ROOT that serve as common, foundational components to almost every other simulation and analysis stack.
These core components are often stored separately from the rest of the software collections available in CVMFS.
Second, there are shared components such as setup scripts, configuration files, and calibration or condition data (required to interpret readings from detectors) that are also nearly universally required but are more particular to a given experiment or group.
Finally, there is the collection of software maintained by each experiment.
Software collections may contain multiple copies of the same software for different architectures and versions.
A single analysis job does not use the majority of the available software.
There is some overlap among these categories of software,
for example some software components are widely enough used to be considered core components,
and usage and availability of components varies over time.

We use these observations about the structure and behavior of applications on CVMFS to build a simplified simulation taking into account dependency requirements for evaluating our container image management scheme.
For the purpose of simulation,
we assume that the requirements of a job are given as discrete components or collections of files.
While Shrinkwrap can operate at the granularity of individual files,
both static analysis and runtime tracing produce file collections to include.
We are intentionally somewhat vague here to allow for both explicitly defined software components and tracing outputs.
We examined the software collections available in the SFT repo and constructed a DAG of dependencies among the packages.
The currently available software collection in the SFT repo consists of NNN packages,
where a program or library typically provides packages for multiple versions and configurations.
When building a projection,
we recursively include dependencies of requested software.
This more closely approximates the structures of actual container images,
while still allowing for randomly selected package requests.
For each simulated image request,
we chose a random selection of packages (either uniform random selection or subject to the process below)
and then added the closure of the package dependencies.
This first image simulation scheme captures the structure inherent in the software collection,
in that packages in addition to those requested are automatically included so as to ensure a functional image.
The initial selection of packages,
however, is simply uniformly random.
This is not necessarily realistic, however,
so we next took usage patterns into account.

We assumed that certain components are included in most container images,
while other components are relatively rarely used.
While multiple versions and variations might exist,
these components have a very high likelihood of appearing in every container image.
These components correspond to the core components, setup scripts, calibration data, etc.\ described previously.
For the purposes of simulation,
we would like to see these components in a large proportion of jobs across all users and experiments.

We obtained CVMFS usage logs from frontend nodes at CERN,
which give rough usage frequencies of the packages in the SFT repo.
To generate simulated container requests,
we made random package selections subject to the observed distribution of package use.
Many packages in the repo were not observed being used at all,
so we added a small, non-zero probability of selecting packages lacking usage data.
We applied this technique in combination with the package dependency graph,
so that images contained a weighted random core selection along with any dependent packages.
This second image simulation scheme takes into account the dependency structure of the repo as well as usage patterns observed in the wild.

As a base for comparison,
we also generated simulated images consisting of packages chosen in a uniform random way.
To ensure that total size (or at least total number of packages)
is comparable to images generated by the previous two methods,
for this approach we started with an image request generated via the first scheme discussed
(uniform random core selection with dependencies added).
We considered only the number of software packages in the resulting image,
and then chose the same number of packages uniformly randomly from the entire repo,
ignoring usage information and package dependencies.
while images generated in this way are not particularly realistic,
they will be useful for evaluating image management schemes.
By comparing results with random images to those with the previous two image generation schemes,
we can compare the general case of containers as collections of arbitrary data to the specific focus of this work,
i.e.\ containers with selections of experimental software.

\subsection{An initial approach}

The simplest approach for automatically projecting container images is to create a suitable image on each batch job submission.
Under this design,
on job submission the bridge machine first checks if the same projection had been requested before.
If so, it arranges to transfer its cached container image to worker nodes that require it.
If the bridge node does not have that projection built already,
it uses Shrinkwrap to create the requested projection,
then invokes mksquashfs to create the container image.
Once the bridge node places the newly created image in its cache,
the process proceeds as in the previous case.
The architecture of this initial approach is shown in figure ???.

Unfortunately, this solution is very wasteful of computing time and storage space.
Even a minimal image consisting of a precise set of required libraries and frameworks ranges in the tens of gigabytes,
depending on the application.
The full-repo images being tested at NERSC are multiple terabytes in size and take the majority of a day to create.
In addition, image creation must be completed before any actual processing occurs,
i.e.\ there is no way to start with a partial image that supports the first steps of a pipeline and finish building the image as the pipeline runs.
There is also the cost of transferring and storing freshly created images on any nodes that will use them,
further delaying job dispatch and increasing load on the network and storage infrastructure.

The disk I/O alone required to create even a targeted image is likely to take more time than a single simulation or analysis job.
Thus a simple performance improvement is to re-use previously created container images for multiple jobs.
A single experiment submitting computational tasks is likely to request the same projection of CVMFS,
at least for jobs submitted around the same time.
It is straightforward to check for exact equality between projections as long as the system retains the set of files that went into their creation.
Note that it is important to retain both the paths and the hash of the file contents used in images.
This ensures that multiple versions of the same file over time are recognized as distinct entities.
If there are few CVMFS users at a site,
this method could be sufficient.

\subsection{Problems with the initial approach}

In developing our initial approach,
we found that a number of real-world usage patterns are poorly handled.
A small change to a projection,
such as adding a library or updating dependencies,
results in the creation of a second, unrelated image.
Even a small change such as removing an unnecessary documentation file results in a completely new container image.
This is a manageable nuisance when manually testing or developing a system,
but potentially results in problematic proliferation of images in a system managing images automatically and receiving updated specifications over time from tracing jobs.

In addition to unnecessary duplication for a single user/application,
there is also significant duplication across multiple users.
In the simulation and analysis pipelines at CERN,
a number of large components such as ROOT are used nearly universally.
These components provide basic functionality and infrastructure that each experiment's software stack builds upon.
Moreover, these same common infrastructure components must be included regardless of user or application.
Thus even if every user of the system only creates a single container image,
there is a duplicate copy of all such common infrastructure components for each user.
This is problematic if we would like to design a system that can support many users at a site.

These two sources of duplication tend to compound each other,
leading us to conclude that a more sophisticated approach is necessary to work with realistically-provisioned workers and system infrastructure.
These issues are fundamentally related to unnecessary duplication of data among container images.
The same types of issues motivate the use of dynamic libraries for everyday applications.
With statically linked executables,
each application includes a complete set of required libraries.
While very stable and reliable,
OS distributions prefer to use dynamic linking.
This allows applications to share a single copy of each library,
so that foundational components such as libc are not copied into every executable,
and so that updates and changes to the set of installed packages do not require rebuilding the entire system.
For this approach to work,
package maintainers must arrange all software packages in the system to ensure compatibility,
and the OS distribution must set policies for system organization, update procedures, etc.
Container images here function similarly to statically linked executables,
but since multiple complete OS distributions are stored within CVMFS,
and since we are dealing with multi-terabyte repos and entire frameworks and software stacks rather than individual OS libraries,
the scale involved is of course larger.
In the case of CVMFS, there is no notion of a ``package mainainer'' or central authority over all software.
Each experiment manages its own software repo as it chooses,
with some components and repos shared between multiple applications.
Since we would like to be able to automatically handle existing applications that are not designed with central management in mind,
we must instead look at other approaches for dealing with data duplication.

\subsection{Deduplicating Container Images}

The representation of projections as disk images is problematic for deduplicating data.
Internally, Shrinkwrap uses hard links to a common data store so that a single file is only stored once across all projections in the system.
On creating disk images, however,
these links are broken.
Each disk image must contain a complete copy of all data,
so that a file must be included (and take up space) in every image that includes it.
Docker's solution is to build container from layers of archives that can be reused.
Docker layers are explicitly defined when writing a Dockerfile.
With CVMFS, however, there is no existing layer-like organization or universal rule that could derive layers for existing applicaions.
Working with layers defined from the outset is also a poor fit if we would like to support partial, gradually improved information from traces.
We therefore desire some sort of deduplication that can be applied for existing applications to an assortment of container images after creation.

To develop a deduplication strategy,
we started with some observations about the projection and image properties.
For ``large'' images,
there is limited opportunity for deduplication.
Each large image is a fairly complete copy of a particular repo.
While there is bound to be some overlap between versions of the same repo,
an updated image will contain a large number of updated software components,
making a ``deduplicated'' between two versions significantly larger than either individually.
There are also likely to be some conflicts in the filesystem if software components re-use the same paths for different files between versions.
This case is akin to combining RHEL~6 and RHEL~7 in an attempt to save space.
While there are similarities between the two,
the significant changes and conflicts make this a poor strategy.
Thus for our strategy we would like to avoid ``deduplicating'' images that have large non-overlapping portions.

Likewise, there is little reason to attempt to deduplicate large images built from different repos.
Such images would have little to no overlap,
so the only benefit to combining the two would be reducing the number of images by one.
This comes at the cost of needlessly storing and transferring the unused half.
This case is akin to bundling a complete Debian installation alongside RHEL.
Our deduplication strategy should only take effect for images with significant overlap.

It is also undesirable to combine images of drastically different size.
As an example, suppose a user creates a small projection as a subset of a repo for running a specific application.
This small image overlaps completely with a large image of the whole repo,
and there is likely no conflict.
Automatically combining the large and small image, however,
can have unintended consequences.
Small, tailored images minimize transfer and storage overhead,
and can be important for operating at scale.
It is therefore unwise to surprise users by suddenly making their container images several orders of magnitude larger.
This is akin to distributing a library exclusively by placing it within a disk image of a Debian installation.
We would therefore like to attempt deduplication only for
\begin{itemize}
\item similarly-sized images.
\item with large overlap.
\item and with no conflicting paths.
\end{itemize}

\subsection{Quantifying Similarity between Projections}

Having set out the conditions under which to consider deduplicating projected images,
we are in a position to define a metric to quantify the similarity between images.
This would allow us to identify projections that are ``close'' (for some definition of close) as candidates for optimization.
We chose the Jaccard distance under appropriate choice of set elements as it conveniently satisfies the desired qualities set out above,
and is very well used and studied.
In our case, we must describe a container image as consisting of a set of (path, content hash) tuples.
CVMFS already stores files by the hashes of their contents,
so this information is readily available.
Considering both the file names and contents is important as CVMFS allows for updates to file contents over time.
We chose fine-grained tracking of file contents rather than simply recording the version of the CVMFS repo to afford additional opportunities for dedupliaction.
Since each update to a repo does not change the contets of every (or even the majority of) files,
data from multiple repo versions may well be able to coexist in a single container image.

For two sets $A$ and $B$,
the Jaccard distance $d_j$ is defined as follows.
\[
d_j(A, B) = 1 - \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cup B| - |A \cap B|}{|A \cup B|}
\]
The Jaccard distance is a metric in the mathematical sense,
making many intitive assumptions about distance and relationships between elements mathematically sound.
For the purposes of comparisons,

For this application in particular,
the Jaccard distance captures several desirable properties of projections.
In the case of two projections that differ only by one file,
the Jaccard distance will be small.
Likewise for a par of projections with nothing in common,
the distance will be high.
And since the Jaccard distance is a proper metric,
related projections will tend to occur in clusters;
if two projections are both found to be extremely close to a third projection,
we can conclude that the two are themselves close by the triangle inequality.
Other choices of metric may well be viable,
but we found the Jaccard distance to be both well-studied and amenable to efficient approximate impelementation.
As described in section ??,
we used MinHash~\cite{minhash} as a constant-time approximation of the Jaccard metric as a first pass at selecting similar images.
Such an approximation is important in practice due to the sizes of the data involved;
metadata listings alone (not including any file data) consumed multiple gigibytes of storage in some of the container images we evaluated.

\subsection{Image Management as an Online Problem}

\begin{figure*}[t]
\centering
\fbox{\includegraphics[width=0.95\textwidth]{drawings/system_architecture.pdf}}
\caption{Architecture of a Site-wide Image Management System}
\label{figure:sys-arch}
\end{figure*}

Combining the tools and techniques introduced so far,
we are now ready to address the central problem in managing projected images as a site-wide service.
Rather than relying on administrators to manually manage images,
we must define a method for making a decision online as to how to efficiently satisfy the dependency requirements for batch jobs.
Our method must be suitable for inclusion as part of the setup for batch job scheduling and must be efficient in terms of both computation time and storage space.
We will extend the initial implementation discussed in section ?? by tracking distances between projections and allowing existing images to be augmented rather than creating new ones for each request.
\Cref{figure:sys-arch} gives an overview of the structure of this system.

Upon receiving a request for a projection,
we first hash the new projection according to the MinHash algorithm.
We always store the hash values computed for every projection we build to allow for fast clustering.
Next, we use MinHash to approximate the Jaccard distance between the new projection and all projections previously built.
This computed distance can be cached to ensure that we estimate the distance between each pair of images only once.
Since comparisons using MinHash take constant time for a given probability bound independent of the size of the sets involved,
we can quickly identify cached projections that are similar to the new request.

To decide if two projections are ``close'',
we define the parameter $\alpha$ as the maximal Jaccard distance between closely related projections.
Since Jaccard distance is by definition between zero and one,
$\alpha$ must be in the same range.
This $\alpha$ parameter is something like the ``globbiness'' of the system.
Small choices of $\alpha$ requires that projections be extremely close before considering them for merging.
In the extreme case with $\alpha = 0$,
only \emph{identical} images will be considered close so no images will be merged.
This results in a larger number of independent images.
Choosing $\alpha$ to be larger makes it more likely for images to be considered similar and merged.
This results in more augmented images that serve multiple tasks.
In the extreme case of $\alpha = 1$,
\emph{every} pair of images is considered close and merged if possible.
This results in large container images that have accumulated many projections.
We explore tradeoffs when choosing $\alpha$ and give recommendations based on observed patterns of activity in section ??.

Having selected a set of candidate projections that are close enough to the new request,
we next check each in turn to find either a superset of the requested projection or one without version incompatibilities.
We first sort the list of candidate images by Jaccard distance.
This ensures that our system prioritizes candidates that are similar in size to the requested image.
In the case of finding a superset,
we can simply use the corresponding cached container image.
Presumably this superset was created by the merging process below,
so we are done.
Otherwise, we continue to look for a compatible projection
As discussed in section ??,
two projections are incompatible if they both include the same path but point to different data.
This situation can arise if for example a library in CVMFS is updated in place.
Old and new projections would include an identical filesystem path,
but the new version would have different contents.
This situation can arise in more subtle ways,
for example with symlinks controlled by environment variables.
Since projections of CVMFS must statically resolve such links,
incompatibilities can arise despite paths appearing compatible under CVMFS.
Any candidate projections that are incompatible with the new request are removed from further consideration.

If no compatible projections were found among the candidates,
we create a container image for the projection as requested and add it to the cache.
Otherwise, on finding a compatible cached projection the next step is to augment it with the contents of the new request.
We choose the first compatible image from the list of candidates for merging,
again prioritizing images that are similar in size and contents.
As previously discussed,
Shrinkwrap can update a projection that was previously created;
since we keep the projection used to generate each container image in the cache,
we can simply augment the selected projection with the new files from the request.
Next, we build a container image based on the newly updated projection.
The computational cost of this step could become high since we are creating larger combined projections.
Appropriate choice of $\alpha$, however,
bounds the distance between the combined projections such that the cost of creating the augmented container is arbitrarily close to cost of simply fulfilling the original request.
Since the augmented projection is the union of two different projections,
we can substitute it for both.
The container image for the augmented projection can replace the previous image,
so that the net increase in storage space comes only from the newly added files.
Since a single augmented image now serves two different batch job submissions,
we avoid making two copies of the common files and data.

One potential issue with this merging strategy is that there is no way to prevent poor merging decisions.
In the case of a one-off submission,
the system might merge an existing, frequently used image.
This is not problematic for the one-off job itself,
but increases overhead for subsequent jobs.
The original, frequently used image would increase in size with no additional benefit to other tasks.
Images might become ``bloated'' in this way,
accumulating infrequently used dependencies and increasing overhead indefinitely for future tasks.
The Jaccard distance metric gives a natural way to capture and quantify this effect.
As an image becomes bloated due to repeated merges,
its distance from every individual request increases.
After sufficient growth,
the image will become too far from any request to even be considered.
Without regular use,
the bloated image will eventually be removed in LRU order.
Choice of $\alpha$ allows an upper limit on the amount of useless bloat in images.

\begin{figure}
\includegraphics[width=\linewidth]{curated/time-series/time-series.pdf}
\label{fig:series}
\if 0
zoomed out?
cache limit, deletes start
merge tracks writes, it's the dominant factor
hits grow gradually as cache merges and fills
deletes help prune useless stuff, remaining should be frequent hits
far ends are either insert or merge, middle alphas have both
\fi
\end{figure}

Figure~\ref{fig:series} shows a single simulation of the image management system,
with $\alpha=0.0.75$, cache size XXX, ...
First, we note that most of the operations are merges.
This makes sense, as this simulation was at fairly high $\alpha$.
The total bytes written also closely tracks merges,
indicating that merging is the dominant source of I/O.
We can still see inserts over the course of the simulation.
At more extreme $\alpha$ values,
we would expect to see one of these operations almost exclusively.
As the data in cache continues to rise,
we eventually see the cache limit,
after which the delete count starts increasing.
Over the course of the simulation,
inserts and deletes are filling and emptying the cache such that it remains close to its storage limit.
We also observe the number of cache hits continue to rise despite deletions.
As we will see, merging allows for a greater proportion of hits even if the amount of data remains constant.
This is because frequently used data can be merged,
reducing duplication.
The cache limit then ensures that infrequently used parts are eventually removed.

\begin{figure}
\includegraphics[width=\linewidth]{curated/time-series/time-series.pdf}
\label{fig:series}
\if 0
zoomed out?
cache limit, deletes start
merge tracks writes, it's the dominant factor
hits grow gradually as cache merges and fills
deletes help prune useless stuff, remaining should be frequent hits
far ends are either insert or merge, middle alphas have both
\fi
\end{figure}

Figure~\ref{fig:series} shows a single simulation of the image management system,
with several notable details.
First, we see that most of the operations are merges.
This makes sense, as this simulation was at fairly high $\alpha$.
The total bytes written also closely tracks merges,
indicating that merging is the dominant source of I/O.
We can still see inserts over the course of the simulation.
At more extreme $\alpha$ values,
we would expect to see one of these operations almost exclusively.
As the data in cache continues to rise,
we eventually see the cache limit,
after which the delete count starts increasing.
Over the course of the simulation,
inserts and deletes are filling and emptying the cache such that it remains close to its storage limit.
We also observe the number of cache hits continue to rise despite deletions.
As we will see, merging allows for a greater proportion of hits even if the amount of data remains constant.
This is because frequently used data can be merged,
reducing duplication.
The cache limit then ensures that infrequently used parts are eventually removed.

\section{Evaluation}

To evaluate the viability of our approach in automatically deduplicating a container image store,
we simulated the behavior of the system over time for a range of $\alpha$ values.
Choice of $\alpha$ is important here to ensure that common components can be detected and merged in container images,
and that old images or poor decisions are eventually removed from the system.
Choosing $\alpha$ too far in either direction will result in excessive storage overhead due to duplicated components,
or excessive compute and network overhead repeatedly merging largely unrelated images.
Our goal in this evaluation is therefore to choose $\alpha$ so as to minimize the storage and compute costs associated with maintaining a collection of images.

Since our simulation uses random simulated requests,
there is noticable variability between individual simulations.
Thus for a given choice of cache size, job count, etc.
we repeated the simulation 20 times and reported the median behavior over the runs.
At each choice of $\alpha$ (in steps of 0.05)
we performed a set of 20 simulated runs,
allowing us to plot various measurements of the system versus $\alpha$.

Sweeping over the range of $\alpha$ values in this way,
we can immediately see differences in the frequency of simulated operations.
Figure~\ref{fig:opcounts} shows the upper range of $\alpha$ values where behavior differences appear.
From the lower values on the left,
the insert and delete counts are the primary (or only) operations,
with number of hits relatively constant.
This corresponds to a simple LRU-based cache.
The insert count is higher due to cache filling,
but the two tend to increase in lockstep.
As $\alpha$ increases,
image merges become more probable.
The merge count in the figure steadily increases throughout most of the upper range,
while inserts and deletes decrease.
This suggests that at high $\alpha$ values,
the cache space would be more efficiently used,
with some of the duplication merged out.
In the extreme case with $\alpha=1$,
every request is merged into a single image in cache,
hence the sudden increase in hit rate and decrease in total merges at the far right of Figure~\ref{fig:opcount}.
Note that as merging increases,
the proportion of hits generally also increases.
This indicates that more of the requests can be satisfied by previously merged images in the cache.
From the perspective of the system,
this is beneficial as there is no extra time or compute cost for handling a larger proportion of images.

\begin{figure}
\includegraphics[width=\linewidth]{curated/comparative/operation_count.pdf}
\label{fig:opcounts}
\if 0
inserts and merges switch as alpha goes up
more hits with merging
allows better responsiveness? image is immediately ready
inserts and deletes go to zero, so storage use will taper off
drop off: we keep everything, so 500 requests get merged; hit count goes up correspondingly since everything is in cache
not gonna be close to cache limit
inserts and deletes track: corresponds to LRU, inserts while filling then balanced
\fi
\end{figure}

\begin{figure}
\includegraphics[width=\linewidth]{curated/comparative/distribution_efficiency.pdf}
\label{fig:dist-eff}
\if 0
explain what happened
metrics: why we care about efficiency?
cache efficiency: gauge duplication (waste), keeping working set costs lots of storage
container efficiency: unexpected overhead for each request; compute, network, etc.
place limits based on site, conditions
limits exist, we won't tell you
maybe just buy more cache?
large scale (10-100 TB), no can do...so need to merge sometimes
why 100 and 20 on the left
why they switch places: extreme cases
strike a balance
one curve might be more important for you
effects are not uniform/balanced gentle drop, massive spike in overhead on right
\fi
\end{figure}

\subsection{Determining a Viable Range for $\alpha$}

When sweeping over the range of $\alpha$ values,
there are a large number of metrics available to summarize each simulation run.
Many, however, are highly coupled with the particular system configuration and difficult to compare as we vary the paramaters of the simulations.

We therefore chose to define two metrics, cache efficiency and container efficiency,
to indicate the relative utilization of the system.
Figure~\ref{fig:dist-eff} plots these two metrics for a representative simulation.
We defined cache efficiency as the ratio of unique data to total data in the cache.
In our case, this is equivalent to the ratio of the size of the unique packages to the total cache size.
If many images contain copies of the same packages,
the cache efficiency decreases.
This metric captures the extent of duplication within the cache across all images.
With no merging there is a high degree of duplication,
so the cache efficiency is low.
On the other end of the spectrum,
maintaining a single, large image containing all data results in cache efficiency of 100\%,
as the entire cache resides in that single image with no duplication

We defined container efficiency as the ratio of the size of the requested container
(in our case a set of requested packages plus all dependencies)
to the size of the container the system actually used for the job.
In the absence of merging,
these two are equal so the container efficiency is 100\%;
jobs are run with exactly what was requested.
By merging to allow for image reuse,
we include additional, unrequested data in container images.
The container efficiency measures this difference between requests and containers.
In the extreme case of $\alpha=1$ with a single large image, for example,
the container efficiency is poor because the entire repo is used for every request,
regardless of size.

These two extreme cases, no merging among many images and a single merged image,
can both be useful in some situations.
If the requirements of all jobs are known in advance and worker storage and bandwidth are sufficient,
maintaining a single, large image for all jobs on all workers is an useful strategy.
After the initial (costly) image creation and transfer,
there are no continuing costs or delays.
When resources are limited or requirements change regularly,
however, this approach becomes prohibitively expensive.
The high compute and bandwidth overhead apply for every image update,
which in the worst case could be every job.

The other extreme situation,
simply caching requests with no merging,
can also be viable.
This approach is simpler and does not come with extreme per-job overhead.
In addition, frequent changes to the jobs and requirements do not degrade the efficiency of each job.
At large scale, however,
the overall system efficiency suffers.
Due to duplication among images,
larger caches contain less and less unique data.
Thus to support a given repo,
it can become necessary to provision a cache much larger than the size of the repo.
With common CVMFS repos consuming several terabytes of storage each,
the amount of cache space used can quickly grow.

These two limiting factors,
the compute and transfer cost in the highly merged case and the cache efficiency in the unmerged case,
serve as limits on the viable range of $\alpha$ values for the system.
Figure~\ref{fig:dist-eff} shows two vertical lines indicating these limits on a plot of efficiencies.
The left line shows a lower limit of around 30\% on the cache efficiency.
Choosing $\alpha$ too low results in cache efficiency reaching below 20\%.
On the right, the line gives an upper limit on the likelihood of merging.
As shown in Figure~\ref{fig:opcounts},
the amount of I/O and compute to update images becomes much larger if $\alpha$ is set too high.
These two limits define a range of viable values for $\alpha$.
There is no general rule for the placement of these limits,
which depends strongly on the performance characteristics of the execution environment,
as well as the priorities of the administrators in optimizing the system.
Rather than defining where these limits fall,
we give per-site considerations in choosing limits in Section ??.

\begin{figure}
\includegraphics[width=\linewidth]{curated/comparative/cache_efficiency.pdf}
\label{fig:cache-eff}
justification for cache efficiency
unique vs total
full x range?
merging can give more unique data in less cache space
\end{figure}

Figure~\ref{fig:cache-eff} shows the actual data sizes used to calculate the cache efficiency.
Without merging, the unique data makes up a small proportion of the cache.
With increased merging, the amount of unique data increases.
For sufficiently high $\alpha$,
merges occur more frequently than cache inserts,
resulting a drop in total storage size.
On the far right of Figure~\ref{fig:cache-eff},
the entire cache contents are merged into a single large image,
so that the unique and total data sizes become equal.
The cache efficiency metric is affected both by the increase in unique data and by the decrease in total cache size.

\begin{figure}
\includegraphics[width=\linewidth]{curated/comparative/container_efficiency.pdf}
\label{fig:container-eff}
\if 0
related to container efficiency: not exact but shows it
too much jitter from individual requests, so cumulative
crossover: hit rate allows reuse, so slightly less bytes written than requested
requested is based on random stream, constant
actual written goes up fast with alpha, so more overhead
proxy for compute/network/etc.
really steep at high alpha
\fi
\end{figure}

Figure~\ref{fig:container-eff} shows the cumulative costs of creating and updating images relative to the cost of simply providing the requested images.
These costs illustrate the effect of container efficiency over the course of a simulated run.
Without merging, the actual I/O and compute costs in the system closely follow the requests.
In Figure~\ref{fig:container-eff} the actual I/O is slightly smaller than the requested amount due to caching:
the system can sometimes reuse images without performing any I/O.
As $\alpha$ increases, the cost of updating and merging images comes to dominate the total cost.
Administrators migh decide, for example,
to spend at most twice the compute and I/O time relative to the the actual requested operations.
This would correspond to the upper compute limit shown on \label{fig:dist-eff}.
Depending on the priorities at a particular execution site,
either the storage utilization or compute and I/O time could be the critical factor in optimizing performance.

\begin{figure}
\includegraphics[width=\linewidth]{curated/comparative/distribution_efficiency_comp.pdf}
\label{fig:comp-eff}
\if 0
different data patterns cause different effects
random has high utilization
pushed right, limited opportunity for merge, since no overlap
merging doesn't help
random is not realistic though
merging is not general solution, but for trees of software, look at this

tree has more duplication, starts merging much sooner
closure is very non-controvertial, but results in serious changes in efficiency
common to any software, so opportunity for optimization
common core that can merge to avoid dups
including usage stats (dist) amplifies these somewhat
real usage should be stronger than dist

use dist from now on because reasonable approximation to actual usage
similar to tree anyway
mild but noticable difference from tree, more natural approximation to real usage
\fi
\end{figure}

We also compared the efficiencies of several types simulated images under a range of $\alpha$ values.
For the previous discussion, synthetic images were generated taking into account both the tree of package dependencies and usage statistics.
Figure~\ref{fig:comp-eff} shows a representative simulation with two other synthetic image types included.
First, we measured images considering only the dependency relationships but not usage.
These images show similar behavior to those previously discussed,
albeit with slightly lower efficiencies.
More realistic images primarily make the same trends more pronounced.
This suggests that even with random, non-realistic requests,
the tree structure of package dependencies is sufficient to produce duplication in the cache.
The other type of synthetic image shown in Figure~\ref{fig:comp-eff} is purely randomly generated.
The set of packages in each image was chosen uniformly randomly from the full SFT repo.
These random images do not take into account usage statistics or package dependencies,
making the software in them entirely nonfunctional.
For the random images,
we observe markedly different cache behavior.
The cache efficiency is much higher,
indicating a greater proportion of unique data in the cache.
Since the image contents were selected purely at random,
this is expected.
The container efficiency is also high over a much larger range of $\alpha$ values.
This is because there is effectively no merging until extremely high $\alpha$.
Since image contents are purely random,
there is no correlation between different images.
Thus, it is much more difficult to find images similar enough to merge until the $\alpha$ value is very lax.
This would indicate that our merging strategy is not applicable to arbitrary collections of data.
Random images show little to no effect for most $\alpha$ values.
Our merging strategy, which takes advantage of duplicated content included as a result of dependencies in software,
would be ill advised for situations that are not known to follow similar patterns of duplication.

\if 0
\subsection{Quantifying Image Management Costs}

The next step in evaluation is to define metrics describing the collection of cached containers.
Here we will define metrics in terms of discrete software components/packages to match the simulated streams,
but the same definitions would apply equally well at the granularity of individual files.
We will suppose that there is a collection of requested container images,
from either our simulated stream or from real user requests based on job analysis and/or dynamic tracing.
It is the responsibility of the image management system to provide a container image satisfying the requirements of each request.
Let the set $C$ be the union of all components in one or more requests.
$C$ provides a lower bound on the amount of space required to serve all requests at the same time.
If the storage space required to hold all of $C$ is greater than the total available storage,
then images serving new requests will requre removal of container image(s) for previous requests.
Since we have been considering image management as an online problem,
we cannot place an upper bound on the sizes of requests and thus require some caching policy
(in our case LRU) to ensure that $C$ is finite and fits available storage.
The large, full-repo images tested at NERSC correspond to single images containing all of $C$.
The image size is very large and any changes/updates are expensive both in terms of compute and bandwidth.
NERSC's approach does, however,
avoid duplication of data as there is a single image that serves all users and applications.

We thus define the multiset $A$ as the union (with repetition) of all components in a set of requested images.
If multiple images request the same component,
it will occur multiple times in $A$.
The size required to store $A$,
including all duplicated elements,
is the storage space that would be required if a separate image were created for each job request.
$A$ captures the amount of (potentially) unnecessary duplication in the collection of images.
The initial approach described in section ?? would have a storage cost of $A$.
Note that it is always the case that $|C| \leq |A|$,
and if $|C| = |A|$ then there are no common components among the requested container images.
While large, full-repo images provide the best storage efficiency, being based solely on $|C|$,
they are expensive and unweildy to use in practice.
The initial approach in section ?? is extremely simple and friendly to use in practice,
but its storage cost is determined by $A$.
Our goal, therefore, in evaluating our improved image management scheme detailed in section ?? is to achieve storage cost as close to $C$ as possible without excessive overhead in terms of compute and network bandwidth to keep the image collection up to date.
We define \emph{cache efficiency} as $|C| / |A|$ to quantify the amount of duplication in the system cache.
With poor cache efficiency,
the available storage for the system cache is wasted on repeated copies of the same data.
Improving cache efficiency allows for more more unique image data within the same storage space,
or equivalently allows the same set of job requests to be served efficiently from a smaller cache.
\fi

\subsection{System-specific Effects of $\alpha$ Threshold}

\begin{figure*}
\includegraphics[width=0.48\linewidth]{curated/sensitivity/container_efficiency_cache_percent_plt.pdf}
\hfill
\includegraphics[width=0.48\linewidth]{curated/sensitivity/cache_efficiency_cache_percent_plt.pdf}
\includegraphics[width=0.48\linewidth]{curated/sensitivity/container_efficiency_request_percent_plt.pdf}
\hfill
\includegraphics[width=0.48\linewidth]{curated/sensitivity/cache_efficiency_request_percent_plt.pdf}
\includegraphics[width=0.48\linewidth]{curated/sensitivity/container_efficiency_uses_percent_plt.pdf}
\hfill
\includegraphics[width=0.48\linewidth]{curated/sensitivity/cache_efficiency_uses_percent_plt.pdf}
\label{fig:sensitivity}
\if 0
[t] "just buy more cache"
efficiency is worse, since cache holds on to lots of questionable data
loads of opportunities to try merges, so we see a lot of unnecessary merges
less force removing bad choices, so more cruft can accumulate in many images
better cache utilization with tiny cache, but of course more thrashing
at scale, bad idea
large cache is too pricey at scale
[m] varying reuse doesn't affect too much
below 100 doesn't seem full/steady state yet
cache isn't settled? so haven't dropped bad merges; more unique data in cache boosts efficiency
[b] high number of requests, repeats don't matter
unique data dominates
hard to tell difference between [m] and [b], so not too sensitive to these
once you hit steady state, jobs count/repeats has little effect on efficiency
general trends of merging, not interesting by themselves
\fi
\end{figure*}

In Figure~\ref{fig:sensitivity}, we plot efficiency curves for a range of simulation conditions.
The left column shows container efficiency,
while the right column shows cache efficiency.
In the first row, the number of jobs and the amount of repetition are constant while the cache size is varied.
In the second row, the number of unique requests is varied while holding the other parameters constant,
and the third row varies the number of times each job is submitted while holding the rest constant.

The size of the cache has an inverse relationship with both the container and cache efficiency.
A larger cache can of course hold a larger number of images,
but since each image contains significant duplicated portions,
increasing cache size tends to decrease cache efficiency.
Conversely, small caches more quickly evict images so that ineffective merges and similar images tend not to remain in cache too long.
A larger cache also allows for more opportunities to merge images,
leading to decreased container efficiency.
When deciding how to handle a request,
a large cache full of images is much more likely to contain an image suitable for merging.
With a small cache, opportunities to merge are much more dependent on the order in which requests happen to come.

The effect of varying the number of unique jobs is less pronounced than the effect of cache size.
Streams of 500 and 1000 unique jobs show nearly indistinguishable behavior,
indicating that by 500 jobs the system has reached a steady state.
Continuing with an arbitrarily long stream should not result in significant performance changes.
It appears that 100 unique jobs were not sufficient to fill the cache and reach a steady state.
In this case the container efficiency is somewhat decreased over a range of $\alpha$ values,
suggesting that some ineffective merges had not made their way out of the cache yet.
The cache efficiency in this case is slightly increased.
This would suggest that before reaching a steady state,
the contents of the cache are more assorted and some unnecessary data remain in cache.

The effect of varying the number of times each unique job is submitted appear minimal at best.
As discussed with the previous row,
a stream of 500 unique images should be sufficient to reach a steady state.
For a sufficiently large cache, therefore,
it appears that the efficiency is determined by the amount of unique data and fairly insensitive to how much those pieces of data are used.
This simulation is somewhat unrealistic in that each job is repeated a uniform number of times.
In real systems, we expect more variation in the frequency of repeated jobs.
We predict that a steady, non-uniform distribution of frequencies would result in the same behavior as shown in the bottom row of Figure~\ref{fig:sensitivity},
while changes in the frequencies of jobs (e.g. a new user starting an experiment)
would disturb the steady state and produce effects similar to those in the middle row.

\if 0
Using these metrics for cost and overhead,
we carried out an evaluation of our container image management scheme as follows.
First, we somewhat arbitrarily chose a storage limit of 1~TB for our simulated image cache.
This is not an unreasonable storage cost for an HPC site;
for the test setup in place at NERSC,
the full repo images already consume multiple terabytes per worker.
We experimented with different storage limits around the same order of magnitude,
but did not find marked differences in the behavioral trends observed below.
The important aspect is that some definite limit exist.
This does not take into account the storage used by Shrinkwrap,
which is automatically deduplicated,
nor does it take into account the metadata cost of creating projections.

The next step was to generate a synthetic stream of requests for container images.
These were generated using the procedure described previously.
To ensure that both the merging strategy and the caching policy come into play,
and to try to capture the steady state behavior of the system,
we continued the stream until the total amount of unique data handled was twice the storage capacity.
Equivalently, the request streams were long enough such that $C$ for the entire stream exceeded 2~TB.
We then tabulated the duplication ratio $d$ for the image cache,
as well as the total amount of data written over the course of the simulation.
Our goal is to minimize both duplication and data written,
which will be proportional to the compute and bandwidth cost of creating and updating container images.
We repeated this simulation W times and computed the averages and standard deviations of these metrics.
We repeated this procedure for a range of $\alpha$ thresholds in the range 0 to 1,
with the results shown in figure ??.

Looking at the results in figure ??,
we can see that small values of $\alpha$ result in the highest level of data duplication,
with $d$ decreasing as $\alpha$ increases.
Since $\alpha=0$ corresponds to the initial approach of creating a unique container image for each job submission,
we expect to see the highest level of duplication there.
Looking at the cost in terms of write activity,
we see that large values of $\alpha$ result in the most write I/O,
and thus compute and bandwidth overhead.
Since $\alpha=1$ allows all requests to be merged into a single large image,
we expect minimal duplication.
This configuration, however,
has substantial overhead for image creation and maintenance.
This is a real complaint from administrators at NERSC testing full-repo images.
The high cost of creation means that such large images can only be infrequently created or updated.
\fi

\subsection{Steps to Determining $\alpha$}
TODO

\section{Related Work}
TODO

\section{Conclusion}
TODO

\section{Reproducibility Data}

In an effort to provide consistent, reproducible results outlined are the
resources utilized in this paper and where they can be found.
Specific commits are mentioned to provide the exact version that was used.


All of these repositories are open source and contain Makefiles
and instructions on how to build and run them.

\section*{Acknowledgment}


\bibliographystyle{ACM-Reference-Format}
\bibliography{otherpapers,cclpapers}

\end{document}
